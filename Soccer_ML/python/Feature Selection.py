#!/usr/bin/env python
# coding: utf-8

# # Feature Selection
# - `SelectKbest`ë¡œ ìµœìƒì˜ Featuresë“¤ì„ ì§€ì •í•´ë³¸ë‹¤.
# - `CrossValidation`ì„ í†µí•´ ì¼ë°˜í™”ë  ê°€ëŠ¥ì„±ì„ ìƒê°í•´ë³¸ë‹¤.
# <aside>
# 
# ### **ğŸ’¡ 4) ë¨¸ì‹ ëŸ¬ë‹ ë°©ì‹ ì ìš© ë° êµì°¨ê²€ì¦**
# 
# ë°ì´í„°ì˜ íƒìƒ‰ê³¼ ì „ì²˜ë¦¬ ì‘ì—…ì´ ëë‚¬ë‹¤ë©´ **ëª¨ë¸ë§ì„ í†µí•´ ë² ì´ìŠ¤ë¼ì¸ê³¼ì˜ ì„±ëŠ¥ ë¹„êµ**ë¥¼ í•´ë´…ë‹ˆë‹¤.
# 
# - Linear / Tree-based / Ensemble ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”. (ë‹¤ì–‘í•˜ê²Œ ì‹œë„í•´ë³´ì‹œëŠ” ê±¸ ì¶”ì²œí•©ë‹ˆë‹¤.)
# - í‰ê°€ì§€í‘œë¥¼ ê³„ì‚° í›„ ë² ì´ìŠ¤ë¼ì¸ê³¼ ë¹„êµí•´ë³´ì„¸ìš”.
# - ì–´ëŠì •ë„ ì„±ëŠ¥ì´ ë‚˜ì™”ë‹¤ë©´, êµì°¨ ê²€ì¦ (ì´í•˜ CV)ì„ í†µí•´ì„œ ì¼ë°˜í™”ë  ê°€ëŠ¥ì„±ì´ ìˆëŠ”ì§€ í™•ì¸í•´ë´…ë‹ˆë‹¤.
# - ëª¨ë¸ ì„±ëŠ¥ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ë°©ë²•ì„ ì ìš©í•´ë³´ì„¸ìš”.
#     - Hyperparameter tuning, etc.
# - ìµœì†Œ 2ê°œ ì´ìƒì˜ ëª¨ë¸ì„ ë§Œë“¤ì–´ì„œ validation ì ìˆ˜ë¥¼ ë³´ê³ í•˜ì„¸ìš”.
# - ìµœì¢… ëª¨ë¸ì˜ test ì ìˆ˜ë¥¼ ë³´ê³ í•˜ì„¸ìš”.
# 
# ### **íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•œ í›„, ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€ë‹µí•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.**
# 
# 1. ëª¨ë¸ì„ í•™ìŠµí•œ í›„ì— ë² ì´ìŠ¤ë¼ì¸ë³´ë‹¤ ì˜ ë‚˜ì™”ë‚˜ìš”? ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?
# 2. ëª¨ë¸ ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´ ì–´ë–¤ ë°©ë²•ì„ ì ìš©í–ˆë‚˜ìš”? ê·¸ ë°©ë²•ì„ ì„ íƒí•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
# 3. ìµœì¢… ëª¨ë¸ì— ê´€í•´ ì„¤ëª…í•˜ì„¸ìš”.
# </aside>

# # Classifier Feature Selection (ë¶„ë¥˜ íŠ¹ì„± ì„ íƒ)

# In[9]:


import pandas as pd
import numpy as np
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt

# ì „ì²´ ì»¬ëŸ¼ ì¶œë ¥í•˜ê¸°
pd.set_option('display.max_columns', None)

warnings.filterwarnings("ignore")

df4 = pd.read_csv('../data/cls_data_4p.csv')
df11 = pd.read_csv('../data/cls_data_11p.csv')


# ### ë°ì´í„° ì „ì²˜ë¦¬

# In[10]:


from sklearn.model_selection import train_test_split 

target = 'position'
train, test = train_test_split(df11, random_state=2, train_size=.75, stratify=df11[target])
train, val = train_test_split(train, random_state=2, train_size=.75, stratify=train[target])
train.shape, val.shape, test.shape


# In[11]:


features = train.columns.drop(target)
# X (features)
X_train = train[features] 
X_val = val[features]
X_test = test[features]

# y (target)
y_train = train[target]
y_val = val[target] 
y_test = test[target]


# In[60]:


# íŒŒì´í”„ë¼ì¸ #
from sklearn.pipeline import make_pipeline, Pipeline

# ì¸ì½”ë” #
from category_encoders import OneHotEncoder
from sklearn.feature_selection import SelectKBest, f_classif, f_regression

# ëª¨ë¸ë§ #
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor

# í‰ê°€ #
from sklearn.metrics import classification_report, plot_confusion_matrix
from sklearn.metrics import mean_absolute_error,r2_score, mean_squared_error


# ### Make Pipeline

# In[16]:


pipe = Pipeline([
        ('encoder', OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True)),
])

pipe_train_enc = pipe.fit_transform(X_train)
pipe_val_enc = pipe.transform(X_val)


# ### SelectKBest

# In[17]:


selector = SelectKBest(score_func=f_classif, k=90)

X_train_selected = selector.fit_transform(pipe_train_enc, y_train) 
X_val_selected = selector.transform(pipe_val_enc)


# In[21]:


features_names = pipe_train_enc.columns 

selected_filter = selector.get_support()

selected_names = features_names[selected_filter]
# print(selected_names)

features_scores = pd.DataFrame(selector.scores_,features_names)


# In[28]:


features_scores.sort_values(by=0,ascending=False).head(25)


# In[29]:


features_scores.sort_values(by=0,ascending=True).head(20)


# In[37]:


selector = SelectKBest(score_func=f_classif, k=80)

pipe = make_pipeline(
    OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
    SelectKBest(score_func=f_classif, k=65)
    # MinMaxScaler()
)
X_train_transform = pipe.fit_transform(X_train, y_train)
X_val_transform = pipe.transform(X_val)


# In[38]:


clf = RandomForestClassifier()
clf.fit(X_train_transform, y_train)
y_pred = clf.predict(X_val_transform)
print(classification_report(y_val, y_pred))

label = ['Attack_C', 'Defence_C', 'Midfielder_C', 'Def_Midfielder', 'GK',
         'Attack_L', 'Defence_L', 'Midfielder_L', 'Attack_R', 'Defence_R', 'Midfielder_R']
fig, ax = plt.subplots(figsize=(15, 15))
plot_confusion_matrix(clf, 
                              X_val_transform, y_val,
                              display_labels = label,
                              cmap = 'Blues',
                              normalize='true',
                              ax = ax)
plt.title('RandomForest Classifier(Position)', fontsize=15);


# ë³„ ì˜ë¯¸ê°€ ì—†ìŒ

# ### CrossValidation

# In[30]:


from sklearn.model_selection import cross_val_score


# In[33]:


target = 'position'
train, test = train_test_split(df11, random_state=2, train_size=.75, stratify=df11[target])
train.shape, test.shape

pipe = Pipeline([
        ('encoder', OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True)),
])

pipe_train_enc = pipe.fit_transform(X_train)
train_features_transform = pipe_train_enc.columns
pipe_test_enc = pipe.transform(X_test)


# In[39]:


pipe = make_pipeline(
    OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
    SimpleImputer(missing_values=np.nan, strategy='mean'),
    RandomForestClassifier()
    # MinMaxScaler()
)

k = 20
scores = cross_val_score(pipe, X_train, y_train, cv = k,
                         scoring='accuracy')

print(f'Accuracy ({k} folds):', scores)


# êµì°¨ ê²€ì¦ ê²°ê³¼, ê¸°ì¡´ RandomForestì—ì„œì˜ ê²°ê³¼(0.62)ì™€ í° ì°¨ì´ê°€ ì—†ëŠ” ê²°ê³¼ê°€ ë‚˜ì™”ìŒ. 

# # Regression Feature Selection (íšŒê·€ íŠ¹ì„± ì„ íƒ)

# In[43]:


df = pd.read_csv('../data/reg_data.csv')
df.drop(columns=['position'], axis=1, inplace=True)


# In[44]:


from sklearn.model_selection import train_test_split 

target = 'value_eur'
train, test = train_test_split(df, random_state=2, train_size=.75)
train, val = train_test_split(train, random_state=2, train_size=.75)
train.shape, val.shape, test.shape


# In[45]:


features = train.columns.drop(target)
# X (features)
X_train = train[features] 
X_val = val[features]
X_test = test[features]

# y (target)
y_train = train[target]
y_val = val[target] 
y_test = test[target]


# In[46]:


pipe = Pipeline([
        ('encoder', OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True)),
])

pipe_train_enc = pipe.fit_transform(X_train)
pipe_val_enc = pipe.transform(X_val)


# In[47]:


selector = SelectKBest(score_func=f_classif, k=90)

X_train_selected = selector.fit_transform(pipe_train_enc, y_train) 
X_val_selected = selector.transform(pipe_val_enc)


# In[48]:


features_names = pipe_train_enc.columns 

selected_filter = selector.get_support()

selected_names = features_names[selected_filter]
# print(selected_names)

features_scores = pd.DataFrame(selector.scores_,features_names)


# In[49]:


features_scores.sort_values(by=0,ascending=False).head(25)


# In[50]:


features_scores.sort_values(by=0,ascending=True).head(25)


# In[53]:


pipe = make_pipeline(
    OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
    SimpleImputer(missing_values=np.nan, strategy='mean'),
    SelectKBest(score_func=f_regression, k=65),
    MinMaxScaler()
)
X_train_transform = pipe.fit_transform(X_train, y_train)
X_val_transform = pipe.transform(X_val)


# In[61]:


reg = RandomForestRegressor()
reg.fit(X_train_transform, y_train)
y_pred = reg.predict(X_val_transform)

print(f"MAE : {mean_absolute_error(y_val, y_pred)}")
print(f"RMSE : {mean_squared_error(y_val, y_pred)**0.5}")
print(f"R2Score : {r2_score(y_val, y_pred)}")


# ### CrossValidation

# In[62]:


pipe = Pipeline([
        ('encoder', OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True)),
])

pipe_train_enc = pipe.fit_transform(X_train)
train_features_transform = pipe_train_enc.columns
pipe_test_enc = pipe.transform(X_test)


# In[64]:


pipe = make_pipeline(
    OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
    SimpleImputer(missing_values=np.nan, strategy='mean'),
    MinMaxScaler(),
    RandomForestRegressor()
)

k = 20
scores = cross_val_score(pipe, X_train, y_train, cv = k,
                         scoring='r2')

print(f'MAE ({k} folds):', scores)


# ì´ ë°ì´í„°ì…‹ì€ ë” ë§ì€ íŠ¹ì„±ì„ ê°€ì§ˆìˆ˜ë¡ ë” ì¢‹ì€ ëª¨ë¸ë¡œ í›ˆë ¨ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤.
