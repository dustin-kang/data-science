#!/usr/bin/env python
# coding: utf-8

# # Model Baseline 
# <aside>
# 
# ### **ğŸ’¡ 4) ë¨¸ì‹ ëŸ¬ë‹ ë°©ì‹ ì ìš© ë° êµì°¨ê²€ì¦**
# 
# ë°ì´í„°ì˜ íƒìƒ‰ê³¼ ì „ì²˜ë¦¬ ì‘ì—…ì´ ëë‚¬ë‹¤ë©´ **ëª¨ë¸ë§ì„ í†µí•´ ë² ì´ìŠ¤ë¼ì¸ê³¼ì˜ ì„±ëŠ¥ ë¹„êµ**ë¥¼ í•´ë´…ë‹ˆë‹¤.
# 
# - Linear / Tree-based / Ensemble ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”. (ë‹¤ì–‘í•˜ê²Œ ì‹œë„í•´ë³´ì‹œëŠ” ê±¸ ì¶”ì²œí•©ë‹ˆë‹¤.)
# - í‰ê°€ì§€í‘œë¥¼ ê³„ì‚° í›„ ë² ì´ìŠ¤ë¼ì¸ê³¼ ë¹„êµí•´ë³´ì„¸ìš”.
# - ì–´ëŠì •ë„ ì„±ëŠ¥ì´ ë‚˜ì™”ë‹¤ë©´, êµì°¨ ê²€ì¦ (ì´í•˜ CV)ì„ í†µí•´ì„œ ì¼ë°˜í™”ë  ê°€ëŠ¥ì„±ì´ ìˆëŠ”ì§€ í™•ì¸í•´ë´…ë‹ˆë‹¤.
# - ëª¨ë¸ ì„±ëŠ¥ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ë°©ë²•ì„ ì ìš©í•´ë³´ì„¸ìš”.
#     - Hyperparameter tuning, etc.
# - ìµœì†Œ 2ê°œ ì´ìƒì˜ ëª¨ë¸ì„ ë§Œë“¤ì–´ì„œ validation ì ìˆ˜ë¥¼ ë³´ê³ í•˜ì„¸ìš”.
# - ìµœì¢… ëª¨ë¸ì˜ test ì ìˆ˜ë¥¼ ë³´ê³ í•˜ì„¸ìš”.
# 
# ### **íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•œ í›„, ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€ë‹µí•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.**
# 
# 1. ëª¨ë¸ì„ í•™ìŠµí•œ í›„ì— ë² ì´ìŠ¤ë¼ì¸ë³´ë‹¤ ì˜ ë‚˜ì™”ë‚˜ìš”? ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?
# 2. ëª¨ë¸ ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´ ì–´ë–¤ ë°©ë²•ì„ ì ìš©í–ˆë‚˜ìš”? ê·¸ ë°©ë²•ì„ ì„ íƒí•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
# 3. ìµœì¢… ëª¨ë¸ì— ê´€í•´ ì„¤ëª…í•˜ì„¸ìš”.
# </aside>

# # Spliting Dataset
# - target ê°’ì„ `value_eur` ìœ¼ë¡œ í•˜ê³  ì„ ìˆ˜ë“¤ì˜ ëŠ¥ë ¥ì„ í†µí•´ ì„ ìˆ˜ë“¤ì˜ ê°€ì¹˜ë¥¼ ì˜ˆì¸¡í•  ì˜ˆì •.

# In[11]:


import pandas as pd
import numpy as np
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt

# ì „ì²´ ì»¬ëŸ¼ ì¶œë ¥í•˜ê¸°
pd.set_option('display.max_columns', None)

warnings.filterwarnings("ignore")

df = pd.read_csv('../data/reg_data.csv')

df.drop(columns=['position'], axis=1, inplace=True)


# In[12]:


df


# In[16]:


from sklearn.model_selection import train_test_split 

# íƒ€ê²Ÿ(target)
target = 'value_eur'


# í›ˆë ¨ ë°ì´í„° ì…‹ê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì…‹ ë¶„ë¦¬
train, test = train_test_split(df, random_state=2, train_size=.75)
# í›ˆë¸ ë°ì´í„°ì…‹ì—ì„œ í›ˆë ¨ë°ì´í„° ì…‹ê³¼ ê²€ì¦ ë°ì´í„° ì…‹ ë¶„ë¦¬
train, val = train_test_split(train, random_state=2, train_size=.75)

# íŠ¹ì„±(features)
features = train.columns.drop(target)


# In[17]:


train.shape, val.shape, test.shape


# In[18]:


# X (features)
X_train = train[features] 
X_val = val[features]
X_test = test[features]

# y (Target)
y_train = train[target]
y_val = val[target] 
y_test = test[target]


# ## Regression Modeling

# In[19]:


get_ipython().system('pip install --upgrade category_encoders')


# In[20]:


get_ipython().system('pip install --upgrade xgboost')


# In[21]:


# íŒŒì´í”„ë¼ì¸ #
from sklearn.pipeline import make_pipeline, Pipeline

# ì¸ì½”ë” #
from category_encoders import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler

# ëª¨ë¸ë§ #
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

# í‰ê°€ #
from sklearn.metrics import mean_absolute_error,r2_score, mean_squared_error


# ### make pipeline
# - `str` ë°ì´í„° ìë£Œí˜•ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì»¬ëŸ¼ì„ 0ê³¼ 1ë¡œ ë°”ê¾¸ëŠ” **OneHotEncoder(ì›í•« ì¸ì½”ë”)**ë¥¼ ì‚¬ìš©í•¨.
# - ì´í›„ í›ˆë ¨ ë°ì´í„°ì…‹ì€ `fit_transform`ì„ ì§„í–‰í•¨.
# - ê²€ì¦, ì‹œí—˜ ë°ì´í„°ì…‹ì€ `transform`ì„ ì§„í–‰í•¨.
# 

# In[22]:


pipe = make_pipeline(
    OneHotEncoder(cols=['preferred_foot','body_type','work_rate_Attaking','work_rate_defensive'], use_cat_names=True),
    # SimpleImputer(missing_values=np.nan, strategy='mean'),
    MinMaxScaler()
)

X_train_transform = pipe.fit_transform(X_train)
X_val_transform = pipe.transform(X_val)
X_test_transform = pipe.transform(X_test)


# ### Linear Regression (ì„ í˜•íšŒê·€)

# In[24]:


reg = LinearRegression()
reg = reg.fit(X_train_transform, y_train)
y_pred = reg.predict(X_val_transform)


# In[25]:


print(f"- MAE : {mean_absolute_error(y_val, y_pred)}")
print(f"- RMSE : {mean_squared_error(y_val, y_pred)**0.5}")
print(f"- R2Score : {r2_score(y_val, y_pred)}")


# In[26]:


import matplotlib.pyplot as plt

fig = plt.figure(figsize=[15,10])
sns.regplot(x=y_val, 
           y=y_pred, 
           fit_reg=True);

plt.title("Linear Regression Plot(Value_EUR)", fontsize=20)
plt.xlabel('Truth', fontsize=14)

plt.ylabel('Prediction', fontsize=14)
plt.show()


# ### Ridge

# In[27]:


reg = Ridge()
reg = reg.fit(X_train_transform, y_train)
y_pred = reg.predict(X_val_transform)

print(f"MAE : {mean_absolute_error(y_val, y_pred)}")
print(f"RMSE : {mean_squared_error(y_val, y_pred)**0.5}")
print(f"R2Score : {r2_score(y_val, y_pred)}")


# In[28]:


fig = plt.figure(figsize=[15,10])
sns.regplot(x=y_val, 
           y=y_pred, 
           fit_reg=True);

plt.title("Ridge Plot(Value_EUR)", fontsize=20)
plt.xlabel('Truth', fontsize=14)

plt.ylabel('Prediction', fontsize=14)
plt.show()


# ### RandomForest (ëœë¤ í¬ë ˆìŠ¤íŠ¸)

# In[29]:


reg = RandomForestRegressor()
reg = reg.fit(X_train_transform, y_train)
y_pred = reg.predict(X_val_transform)

print(f"MAE : {mean_absolute_error(y_val, y_pred)}")
print(f"RMSE : {mean_squared_error(y_val, y_pred)**0.5}")
print(f"R2Score : {r2_score(y_val, y_pred)}")


# ### XGBoost

# In[2]:


reg = XGBRegressor()
reg = reg.fit(X_train_transform, y_train)
y_pred = reg.predict(X_val_transform)

print(f"MAE : {mean_absolute_error(y_val, y_pred)}")
print(f"RMSE : {mean_squared_error(y_val, y_pred)**0.5}")
print(f"R2Score : {r2_score(y_val, y_pred)}")

"""
[23:20:11] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
MAE : 427093.0390904744
RMSE : 1251733.4461512708
R2Score : 0.949373346492528
"""


# ## ê²°ë¡ ì ìœ¼ë¡œ
# ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì´ ê°€ì¥ ì ì€ ì˜¤ì°¨ë¥¼ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤. 
