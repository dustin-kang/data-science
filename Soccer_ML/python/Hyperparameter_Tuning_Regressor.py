#!/usr/bin/env python
# coding: utf-8

# <a href="https://colab.research.google.com/github/dustin-kang/Proj2_SoccerPlayer-Machine-Learning/blob/main/Report/7_Hyperparameter_Tuning_Regressor_except_overall.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# # Hyperparameter_Tuning_Regressor
# 
# íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ í†µí•´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ê¸° ìœ„í•´ ëª¨ë¸ì„ ì¡°ì •í•œë‹¤.
# 
# 
# <aside>
# 
# ### **ğŸ’¡ 4) ë¨¸ì‹ ëŸ¬ë‹ ë°©ì‹ ì ìš© ë° êµì°¨ê²€ì¦**
# 
# ë°ì´í„°ì˜ íƒìƒ‰ê³¼ ì „ì²˜ë¦¬ ì‘ì—…ì´ ëë‚¬ë‹¤ë©´ **ëª¨ë¸ë§ì„ í†µí•´ ë² ì´ìŠ¤ë¼ì¸ê³¼ì˜ ì„±ëŠ¥ ë¹„êµ**ë¥¼ í•´ë´…ë‹ˆë‹¤.
# 
# - Linear / Tree-based / Ensemble ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”. (ë‹¤ì–‘í•˜ê²Œ ì‹œë„í•´ë³´ì‹œëŠ” ê±¸ ì¶”ì²œí•©ë‹ˆë‹¤.)
# - í‰ê°€ì§€í‘œë¥¼ ê³„ì‚° í›„ ë² ì´ìŠ¤ë¼ì¸ê³¼ ë¹„êµí•´ë³´ì„¸ìš”.
# - ì–´ëŠì •ë„ ì„±ëŠ¥ì´ ë‚˜ì™”ë‹¤ë©´, êµì°¨ ê²€ì¦ (ì´í•˜ CV)ì„ í†µí•´ì„œ ì¼ë°˜í™”ë  ê°€ëŠ¥ì„±ì´ ìˆëŠ”ì§€ í™•ì¸í•´ë´…ë‹ˆë‹¤.
# - ëª¨ë¸ ì„±ëŠ¥ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ë°©ë²•ì„ ì ìš©í•´ë³´ì„¸ìš”.
#     - Hyperparameter tuning, etc.
# - ìµœì†Œ 2ê°œ ì´ìƒì˜ ëª¨ë¸ì„ ë§Œë“¤ì–´ì„œ validation ì ìˆ˜ë¥¼ ë³´ê³ í•˜ì„¸ìš”.
# - ìµœì¢… ëª¨ë¸ì˜ test ì ìˆ˜ë¥¼ ë³´ê³ í•˜ì„¸ìš”.
# 
# ### **íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•œ í›„, ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€ë‹µí•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.**
# 
# 1. ëª¨ë¸ì„ í•™ìŠµí•œ í›„ì— ë² ì´ìŠ¤ë¼ì¸ë³´ë‹¤ ì˜ ë‚˜ì™”ë‚˜ìš”? ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?
# 2. ëª¨ë¸ ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´ ì–´ë–¤ ë°©ë²•ì„ ì ìš©í–ˆë‚˜ìš”? ê·¸ ë°©ë²•ì„ ì„ íƒí•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
# 3. ìµœì¢… ëª¨ë¸ì— ê´€í•´ ì„¤ëª…í•˜ì„¸ìš”.
# </aside>

# In[25]:


import pandas as pd
import numpy as np
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt

# ì „ì²´ ì»¬ëŸ¼ ì¶œë ¥í•˜ê¸°
pd.set_option('display.max_columns', None)

warnings.filterwarnings("ignore")

df = pd.read_csv('../data/reg_data.csv')

df.drop(columns=['position','overall'], axis=1, inplace=True)


# ## 1. Train, Test, Split

# In[26]:


df['value_eur'][df['value_eur'].isna()] = df['value_eur'].median()


# In[27]:


from sklearn.model_selection import train_test_split 

target = 'value_eur'
train, test = train_test_split(df, random_state=2, train_size=.75)
train, val = train_test_split(train, random_state=2, train_size=.75)
train.shape, val.shape, test.shape


# In[28]:


features = train.columns.drop(target)

# X (Features)
X_train = train[features] 
X_test = test[features]
y_train = train[target]

# y (Target)
X_val = val[features]
y_val = val[target] 
y_test = test[target]


# ## pre-OneHot Encoding

# In[29]:


# preprocessing

from sklearn.pipeline import make_pipeline, Pipeline
from category_encoders import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler

# Classifier Model
from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error,r2_score, mean_squared_error

# Model Selection, Parameter Tuning
from scipy.stats import randint, uniform
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV


# In[30]:


pipe = make_pipeline(
                      OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
                      SimpleImputer(missing_values=np.nan, strategy='mean'),
                      MinMaxScaler(),
)

X_train_transformed = pipe.fit_transform(X_train)
X_val_transformed = pipe.transform(X_val)
X_test_transformed = pipe.transform(X_test)


# ## GridSearchCV
# 
# - GridSearch : ëª¨ë¸ë§ì‹œ í•„ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•  ë•Œ ê°€ì¥ ìµœì ì˜ íŒŒë¦¬ë¯¸í„°ê°’ì„ ì°¾ì•„ì£¼ëŠ” ë°©ë²• 
# - ì¶”ê°€ì ìœ¼ë¡œ êµì°¨ê²€ì¦(Cross Validation)ê¹Œì§€ ìˆ˜í–‰

# ### 1

# In[32]:


# Modeling

rf = RandomForestRegressor()
# param = {'n_estimators' : range(50, 600, 10)}
param = {'n_estimators' : [340, 350, 360, 370]}

reg = GridSearchCV(rf,
                   param_grid = param,
                   n_jobs=-1,
                   #cv=5, # í•˜ë‚˜ì˜ íŒŒë¼ë¯¸í„° ìŒìœ¼ë¡œ ëª¨ë¸ë§í•  ë•Œ, train, test êµì°¨ê²€ì¦ì„ 5ë²ˆ ì‹¤í–‰.
                   scoring="neg_mean_squared_error",
                   refit=True, # GridSearch ì§„í–‰ í›„ ìµœê³ ì˜ íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµ.
                   verbose=2)

# í•™ìŠµ ìˆ˜í–‰
reg.fit(X_train_transformed, y_train)


# In[ ]:


y_pred = reg.predict(X_val_transformed)
best_score = -reg.best_score_
best_score ** 0.5


# In[ ]:


r2_score(y_val, y_pred)


# In[ ]:


y_pred = reg.predict(X_train_transformed)
r2_score(y_train, y_pred)


# In[ ]:


reg.best_params_


# In[ ]:


reg.best_estimator_


# In[ ]:


# ê° íŒŒë¼ë¯¸í„°ê°’ë“¤ì— ëŒ€í•œ ëª¨ë¸ ê²°ê³¼ê°’ë“¤ì´ cv_results_ ê°ì²´ì— í• ë‹¹ë¨
report = pd.DataFrame(reg.cv_results_).sort_values(by='rank_test_score').T
report

# score ê²°ê³¼ê°’(ndarrayí˜•íƒœë¡œ í• ë‹¹ë¨) ì¤‘ íŠ¹ì • ì¹¼ëŸ¼ë“¤ë§Œ ê°€ì ¸ì˜¤ê¸° 
# scores_df[['params', 'mean_test_score', 'rank_test_score', 
#            'split0_test_score', 'split1_test_score', 'split2_test_score']]


# ### 2

# In[ ]:


# Modeling

xgb = XGBRegressor(random_state=25)
dists = { " n_estimators": range(340,400, 5)}

reg = GridSearchCV(xgb, dists, n_jobs=-1, scoring="neg_mean_squared_error", verbose=2)

reg.fit(X_train_transformed, y_train)


# In[ ]:


y_pred = reg.predict(X_val_transformed)
best_score = -reg.best_score_
best_score ** 0.5


# In[ ]:


r2_score(y_val, y_pred)


# In[ ]:


y_pred = reg.predict(X_train_transformed)
r2_score(y_train, y_pred)


# In[ ]:


reg.best_params_


# In[ ]:


report = pd.DataFrame(reg.cv_results_).sort_values(by='rank_test_score').T
report


# ### 3

# In[ ]:


xgb = XGBRegressor(random_state=25)
dists = { " n_estimators": [340, 350, 1]}

reg = GridSearchCV(xgb, dists, n_jobs=-1, scoring="neg_mean_squared_error", verbose=2)

reg.fit(X_train_transformed, y_train)


# In[ ]:


reg.best_estimator_


# In[ ]:


y_pred = reg.predict(X_test_transformed)
print(f'MSE : {mean_squared_error(y_test, y_pred)}')
print(f'MSE : {mean_squared_error(y_test, y_pred) ** 0.5}')
print('-------------')
print(f'í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì ìˆ˜ : {r2_score(y_test, y_pred)}')

y_pred_train = reg.predict(X_train_transformed)
print(f'í›ˆë ¨ ì„¸íŠ¸ ì ìˆ˜ : {r2_score(y_train, y_pred_train)}')


fig = plt.figure(figsize=[15,10])
sns.regplot(x=y_test, 
           y=y_pred, 
           fit_reg=True);

plt.title("XGBRegressor(Value_EUR)", fontsize=20)
plt.xlabel('Truth', fontsize=14)

plt.ylabel('Prediction', fontsize=14)
plt.show()


# # Permutation importances
# <aside>
# 
# ###ğŸ’¡ **5) ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•´ì„**
# 
# í”„ë¡œì íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆëŠ” ë¶€ë¶„ ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” SHAP, PDP ë“±ì„ í†µí•´ì„œ ëª¨ë¸ì´ ê´€ì¸¡ì¹˜ë¥¼ ì–´ë–¤ íŠ¹ì„±ì„ í™œìš©í–ˆê±°ë‚˜, ì–´ë–¤ íŠ¹ì„±ì´ íƒ€ê²Ÿì— ì˜í–¥ì„ ë¼ì³¤ëŠ”ì§€ ë“±ì„ í•´ì„í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ë°°ì› ìŠµë‹ˆë‹¤.
# ì—¬ëŸ¬ë¶„ì˜ í”„ë¡œì íŠ¸ì—ë„ ì´ëŸ¬í•œ í•´ì„ ë°©ë²•ì„ í™œìš©í•´ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ë¹„ì „ë¬¸ê°€ë¼ë„ ì¡°ê¸ˆ ë” ì‰½ê²Œ ì´í•´í•˜ê³  ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ì…”ì•¼ í•©ë‹ˆë‹¤.
# 
# - PDP, SHAPì„ í™œìš©í•˜ì—¬ ìµœì¢… ëª¨ë¸ì„ ì„¤ëª…í•©ë‹ˆë‹¤
# - ì‹œê°í™”ëŠ” "ì„¤ëª…"ì´ ì œì¼ ì¤‘ìš”í•©ë‹ˆë‹¤.
# 
# ### **íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•œ í›„, ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€ë‹µí•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.**
# 
# 1. ëª¨ë¸ì´ ê´€ì¸¡ì¹˜ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œ ì–´ë–¤ íŠ¹ì„±ì„ í™œìš©í–ˆë‚˜ìš”?
# 2. ì–´ë–¤ íŠ¹ì„±ì´ ìˆë‹¤ë©´ ëª¨ë¸ì˜ ì˜ˆì¸¡ì— ë„ì›€ì´ ë ê¹Œìš”? í•´ë‹¹ íŠ¹ì„±ì€ ì–´ë–»ê²Œ êµ¬í•  ìˆ˜ ìˆì„ê¹Œìš”?
# 
# train_features_transform
# </aside>

# ### Eli5

# In[9]:


get_ipython().system('pip install eli5')


# In[10]:


pipe = Pipeline([
                 ('preprocessing', make_pipeline(
                     OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
                     SimpleImputer(missing_values=np.nan, strategy='mean'),
                     MinMaxScaler()
                 )),
                 ('reg',  XGBRegressor(random_state=2, n_estmators=50))
],verbose=1)

pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)


# In[11]:


enc = OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True)
X_train_t = enc.fit_transform(X_train, y_train)
X_val_t = enc.transform(X_val)
X_test_t = enc.transform(X_test)
test_features_transform = X_test_t.columns


# In[12]:


import eli5
from eli5.sklearn import PermutationImportance

permuter = PermutationImportance(
    pipe.named_steps['reg'],
    scoring='r2',
    n_iter=5,
    random_state=2
)

X_test_transformed = pipe.named_steps['preprocessing'].transform(X_test)
permuter.fit(X_val_transformed, y_val);

enc = OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True)
X_train_t = enc.fit_transform(X_train, y_train)
X_test_t = enc.transform(X_test)
X_val_t = enc.transform(X_val)

feature_names = test_features_transform.tolist() #Xvalì˜ ì»¬ëŸ¼ *ë¦¬ìŠ¤íŠ¸í™”*
pd.Series(permuter.feature_importances_, feature_names).sort_values()


# In[13]:


# íŠ¹ì„±ë³„ score í™•ì¸
eli5.show_weights(
    permuter, 
    top=None, # top n ì§€ì • ê°€ëŠ¥, None ì¼ ê²½ìš° ëª¨ë“  íŠ¹ì„± 
    feature_names=feature_names # list í˜•ì‹ìœ¼ë¡œ ë„£ì–´ì•¼ í•©ë‹ˆë‹¤
)


# In[14]:


print('íŠ¹ì„± ì‚­ì œ ì „:', X_train.shape, X_val.shape)

minimum_importance = 0.000 # ìµœì†Œ ì¤‘ìš”ë„ (ìµœì†Œ ì´ì •ë„ ì´ìƒì€ ë„˜ì–´ì•¼ í•œë‹¤.)
mask = permuter.feature_importances_ > minimum_importance
features = test_features_transform[mask]

X_train_selected = X_train_t[features]
X_val_selected = X_val_t[features]
X_test_selected = X_test_t[features]

print('íŠ¹ì„± ì‚­ì œ í›„:', X_train_selected.shape, X_val_selected.shape)


# In[17]:


X_train_selected
import seaborn as sns


# In[18]:


pipe = Pipeline([
                 ('preprocessing', make_pipeline(
#                     OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
                     SimpleImputer(missing_values=np.nan, strategy='mean'),
                     MinMaxScaler()
                 )),
                 ('reg',  XGBRegressor(random_state=2, n_estmators=50))
],verbose=1)

pipe.fit(X_train_selected, y_train)
y_pred = pipe.predict(X_test_selected)

print(f'MSE : {mean_squared_error(y_test, y_pred)}')
print(f'MSE : {mean_squared_error(y_test, y_pred) ** 0.5}')
print('-------------')
print(f'í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì ìˆ˜ : {r2_score(y_test, y_pred)}')

y_pred_train = pipe.predict(X_train_selected)
print(f'í›ˆë ¨ ì„¸íŠ¸ ì ìˆ˜ : {r2_score(y_train, y_pred_train)}')

fig = plt.figure(figsize=[15,10])
sns.regplot(x=y_test, 
           y=y_pred, 
           fit_reg=True);

plt.title("XGBRegressor(Value_EUR)", fontsize=20)
plt.xlabel('Truth', fontsize=14)

plt.ylabel('Prediction', fontsize=14)
plt.show()


# In[19]:


X_test_selected


# In[20]:


dataset = pd.DataFrame(X_test_transformed.tolist())


# In[21]:


dataset.columns =test_features_transform


# In[22]:


# from pdpbox.pdp import pdp_interact, pdp_interact_plot


enc = OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True)
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
scaler = MinMaxScaler()

reg = XGBRegressor(random_state=2, n_estimators=50)

X_train_transformed = enc.fit_transform(X_train)
X_test_transformed = enc.transform(X_test)
X_train_imputed = imp.fit_transform(X_train_transformed)
X_test_imputed = imp.transform(X_test_transformed)
X_train_scaled = scaler.fit_transform(X_train_imputed)
X_test_scaled = scaler.fit_transform(X_test_imputed)

reg.fit(X_train_scaled, y_train)
y_pred = reg.predict(X_test_scaled)


# ### SHAP

# In[23]:


get_ipython().system('pip install shap')


# In[24]:


row = dataset.iloc[[250]]  # ì¤‘ì²© bracketsì„ ì‚¬ìš©í•˜ë©´ ê²°ê³¼ë¬¼ì´ DataFrameì…ë‹ˆë‹¤
row


# In[25]:


import shap
explainer = shap.TreeExplainer(reg) 
shap.initjs() #ì¸ë¼ì¸ ìë°”ìŠ¤í¬ë¦½íŠ¸

shap.force_plot(
    base_value=explainer.expected_value,  # ë³´í†µ í‰ê· ê°’ìœ¼ë¡œ ì¡ëŠ”ë‹¤.
    shap_values=explainer.shap_values(row), # Shap Value ê³„ì‚°
    features=row
)


# In[32]:


shap_values = explainer.shap_values(dataset.iloc[:500])
shap.summary_plot(shap_values, dataset.iloc[:500], max_display=5)


# In[33]:


shap.summary_plot(shap_values, dataset.iloc[:500], plot_type="bar", max_display=6)

