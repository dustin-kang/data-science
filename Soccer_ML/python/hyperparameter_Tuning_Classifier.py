#!/usr/bin/env python
# coding: utf-8

# <a href="https://colab.research.google.com/github/dustin-kang/Proj2_SoccerPlayer-Machine-Learning/blob/main/Report/6_Hyperparameter_Tuning_Classifier.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# # Hyperparameter_Tuning_Classifier
# 
# íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ í†µí•´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ê¸° ìœ„í•´ ëª¨ë¸ì„ ì¡°ì •í•œë‹¤.
# 
# 
# <aside>
# 
# ### **ğŸ’¡ 4) ë¨¸ì‹ ëŸ¬ë‹ ë°©ì‹ ì ìš© ë° êµì°¨ê²€ì¦**
# 
# ë°ì´í„°ì˜ íƒìƒ‰ê³¼ ì „ì²˜ë¦¬ ì‘ì—…ì´ ëë‚¬ë‹¤ë©´ **ëª¨ë¸ë§ì„ í†µí•´ ë² ì´ìŠ¤ë¼ì¸ê³¼ì˜ ì„±ëŠ¥ ë¹„êµ**ë¥¼ í•´ë´…ë‹ˆë‹¤.
# 
# - Linear / Tree-based / Ensemble ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”. (ë‹¤ì–‘í•˜ê²Œ ì‹œë„í•´ë³´ì‹œëŠ” ê±¸ ì¶”ì²œí•©ë‹ˆë‹¤.)
# - í‰ê°€ì§€í‘œë¥¼ ê³„ì‚° í›„ ë² ì´ìŠ¤ë¼ì¸ê³¼ ë¹„êµí•´ë³´ì„¸ìš”.
# - ì–´ëŠì •ë„ ì„±ëŠ¥ì´ ë‚˜ì™”ë‹¤ë©´, êµì°¨ ê²€ì¦ (ì´í•˜ CV)ì„ í†µí•´ì„œ ì¼ë°˜í™”ë  ê°€ëŠ¥ì„±ì´ ìˆëŠ”ì§€ í™•ì¸í•´ë´…ë‹ˆë‹¤.
# - ëª¨ë¸ ì„±ëŠ¥ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ë°©ë²•ì„ ì ìš©í•´ë³´ì„¸ìš”.
#     - Hyperparameter tuning, etc.
# - ìµœì†Œ 2ê°œ ì´ìƒì˜ ëª¨ë¸ì„ ë§Œë“¤ì–´ì„œ validation ì ìˆ˜ë¥¼ ë³´ê³ í•˜ì„¸ìš”.
# - ìµœì¢… ëª¨ë¸ì˜ test ì ìˆ˜ë¥¼ ë³´ê³ í•˜ì„¸ìš”.
# 
# ### **íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•œ í›„, ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€ë‹µí•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.**
# 
# 1. ëª¨ë¸ì„ í•™ìŠµí•œ í›„ì— ë² ì´ìŠ¤ë¼ì¸ë³´ë‹¤ ì˜ ë‚˜ì™”ë‚˜ìš”? ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?
# 2. ëª¨ë¸ ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´ ì–´ë–¤ ë°©ë²•ì„ ì ìš©í–ˆë‚˜ìš”? ê·¸ ë°©ë²•ì„ ì„ íƒí•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
# 3. ìµœì¢… ëª¨ë¸ì— ê´€í•´ ì„¤ëª…í•˜ì„¸ìš”.
# </aside>

# In[11]:


import pandas as pd
import numpy as np
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt

# ì „ì²´ ì»¬ëŸ¼ ì¶œë ¥í•˜ê¸°
pd.set_option('display.max_columns', None)

warnings.filterwarnings("ignore")

df4 = pd.read_csv('../data/cls_data_4p.csv')
df11 = pd.read_csv('../data/cls_data_11p.csv')


# ## 1. Train, Test, Split

# In[12]:


from sklearn.model_selection import train_test_split 

target = 'position'
train, test = train_test_split(df11, random_state=2, train_size=.75, stratify=df11[target])
train, val = train_test_split(train, random_state=2, train_size=.75, stratify=train[target])
train.shape, val.shape, test.shape


# In[13]:


features = train.columns.drop(target)
# X (features)
X_train = train[features] 
X_val = val[features]
X_test = test[features]

# y (Target)
y_train = train[target]
y_val = val[target] 
y_test = test[target]


# ## pre-OneHot Encoding

# In[14]:


# preprocessing

from sklearn.pipeline import make_pipeline, Pipeline
from category_encoders import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler

# Classifier Model
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, plot_confusion_matrix, f1_score, accuracy_score

# Model Selection, Parameter Tuning
from scipy.stats import randint, uniform
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV


# In[15]:


enc = OneHotEncoder(cols = [
                            'preferred_foot',
                            'body_type'
], use_cat_names=True)

enc_train = enc.fit_transform(X_train)
train_features_transform = enc_train.columns # ì¸ì½”ë”©ì´ ì™„ë£Œëœ íŠ¹ì„±ì„ ë³€ìˆ˜ì— ë‹´ê¸°

enc_val = enc.transform(X_val)
enc_test = enc.transform(X_test)


# ## RandomizedSearchCV
# https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=qbxlvnf11&logNo=221574025920

# In[ ]:


# Modeling

dists = { # Randomized Classifier Parameter
  'randomforestclassifier__n_estimators' : randint(50, 500),
  'randomforestclassifier__max_depth' : [5, 10, 15, 20, None],
  'randomforestclassifier__min_samples_leaf' : [5, 10, 15, 20, None],
  'randomforestclassifier__min_samples_split' : [5, 15, 30, 40, None]
}


pipe = make_pipeline( # Modeling Pipeline
    OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
    SimpleImputer(missing_values=np.nan, strategy='mean'),
    RandomForestClassifier(random_state=2)
)

clf = RandomizedSearchCV( # RamdommizedSearchCV
    pipe, 
    param_distributions=dists, 
    n_iter=50, 
    cv=3, 
    scoring='accuracy',  
    verbose=1,
    n_jobs=-1
)


clf.fit(X_train, y_train)

y_pred = clf.predict(X_val)

print(classification_report(y_val, y_pred))

# ì‹œê°í™”
label = ['Attack_C', 'Defence_C', 'Midfielder_C', 'Def_Midfielder', 'GK',
         'Attack_L', 'Defence_L', 'Midfielder_L', 'Attack_R', 'Defence_R', 'Midfielder_R']
fig, ax = plt.subplots(figsize=(15, 15))
plot_confusion_matrix(clf, 
                              X_val, y_val,
                              display_labels = label,
                              cmap = 'Blues',
                              normalize='true',
                              ax = ax)
plt.title('Random Forest Classifier (Position)', fontsize=15);


# In[ ]:


clf.best_params_


# In[ ]:


clf.best_score_


# In[ ]:


pd.DataFrame(clf.cv_results_).sort_values(by='rank_test_score').T


# In[ ]:


# Modeling

dists = { # Randomized Classifier Parameter
  'randomforestclassifier__n_estimators' : randint(50, 500),
  'randomforestclassifier__max_depth' : randint(10,25),
  'randomforestclassifier__min_samples_leaf' : randint(5,10),
  'randomforestclassifier__min_samples_split' : [5, 10, 15]
}


pipe = make_pipeline( # Modeling Pipeline
    OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
    SimpleImputer(missing_values=np.nan, strategy='mean'),
    RandomForestClassifier(random_state=2)
)

clf = RandomizedSearchCV( # RamdommizedSearchCV
    pipe, 
    param_distributions=dists, 
    n_iter=50, 
    cv=3, 
    scoring='accuracy',  
    verbose=1,
    n_jobs=-1
)


clf.fit(X_train, y_train)

y_pred = clf.predict(X_val)

print(classification_report(y_val, y_pred))

# ì‹œê°í™”
label = ['Attack_C', 'Defence_C', 'Midfielder_C', 'Def_Midfielder', 'GK',
         'Attack_L', 'Defence_L', 'Midfielder_L', 'Attack_R', 'Defence_R', 'Midfielder_R']
fig, ax = plt.subplots(figsize=(15, 15))
plot_confusion_matrix(clf, 
                              X_val, y_val,
                              display_labels = label,
                              cmap = 'Blues',
                              normalize='true',
                              ax = ax)
plt.title('Random Forest Classifier (Position)', fontsize=15);


# In[ ]:


clf.best_params_


# In[ ]:


clf.best_score_


# In[ ]:


pd.DataFrame(clf.cv_results_).sort_values(by='rank_test_score').T


# In[ ]:


# Modeling

dists = { # Randomized Classifier Parameter
  'randomforestclassifier__n_estimators' : randint(370, 430),
  'randomforestclassifier__max_depth' : randint(14,24),
  'randomforestclassifier__min_samples_leaf' : randint(5,10),
  'randomforestclassifier__min_samples_split' : randint(5, 10)
}


pipe = make_pipeline( # Modeling Pipeline
    OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
    SimpleImputer(missing_values=np.nan, strategy='mean'),
    RandomForestClassifier(random_state=2)
)

clf = RandomizedSearchCV( # RamdommizedSearchCV
    pipe, 
    param_distributions=dists, 
    n_iter=50, 
    cv=3, 
    scoring='accuracy',  
    verbose=1,
    n_jobs=-1
)


clf.fit(X_train, y_train)

y_pred = clf.predict(X_val)

print(classification_report(y_val, y_pred))

# ì‹œê°í™”
label = ['Attack_C', 'Defence_C', 'Midfielder_C', 'Def_Midfielder', 'GK',
         'Attack_L', 'Defence_L', 'Midfielder_L', 'Attack_R', 'Defence_R', 'Midfielder_R']
fig, ax = plt.subplots(figsize=(15, 15))
plot_confusion_matrix(clf, 
                              X_val, y_val,
                              display_labels = label,
                              cmap = 'Blues',
                              normalize='true',
                              ax = ax)
plt.title('Random Forest Classifier (Position)', fontsize=15);


# In[ ]:


clf.best_score_


# In[ ]:


clf.best_estimator_


# In[ ]:


model = clf.best_estimator_


# In[ ]:


pd.DataFrame(clf.cv_results_).sort_values(by='rank_test_score').T


# In[ ]:


model = clf.best_estimator_

y_val_pred = model.predict(X_val)
y_train_pred = model.predict(X_train)

print('í›ˆë ¨ f1 score (micro): ', f1_score(y_train, y_train_pred, average='micro'))
print('ê²€ì¦ f1 score (micro): ', f1_score(y_val, y_val_pred, average='micro'))

print('í›ˆë ¨ f1 score (macro): ', f1_score(y_train, y_train_pred, average='macro'))
print('ê²€ì¦ f1 score (macro): ', f1_score(y_val, y_val_pred, average='macro'))


# In[ ]:


# Modeling

dists = { # Randomized Classifier Parameter
  'randomforestclassifier__n_estimators' : randint(380, 410),
  'randomforestclassifier__max_depth' : randint(18,24),
  'randomforestclassifier__min_samples_leaf' : [5,6],
  'randomforestclassifier__min_samples_split' : randint(5, 10)
}


pipe = make_pipeline( # Modeling Pipeline
    OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
    SimpleImputer(missing_values=np.nan, strategy='mean'),
    RandomForestClassifier(random_state=2)
)

clf = RandomizedSearchCV( # RamdommizedSearchCV
    pipe, 
    param_distributions=dists, 
    n_iter=10, 
    cv=3, 
    scoring='accuracy',  
    verbose=1,
    n_jobs=-1
)


clf.fit(X_train, y_train)

y_pred = clf.predict(X_val)

print(classification_report(y_val, y_pred))

# ì‹œê°í™”
label = ['Attack_C', 'Defence_C', 'Midfielder_C', 'Def_Midfielder', 'GK',
         'Attack_L', 'Defence_L', 'Midfielder_L', 'Attack_R', 'Defence_R', 'Midfielder_R']
fig, ax = plt.subplots(figsize=(15, 15))
plot_confusion_matrix(clf, 
                              X_val, y_val,
                              display_labels = label,
                              cmap = 'Blues',
                              normalize='true',
                              ax = ax)
plt.title('Random Forest Classifier (Position)', fontsize=15);


# In[ ]:


model = clf.best_estimator_

y_val_pred = model.predict(X_val)
y_train_pred = model.predict(X_train)

print('í›ˆë ¨ f1 score (micro): ', f1_score(y_train, y_train_pred, average='micro'))
print('ê²€ì¦ f1 score (micro): ', f1_score(y_val, y_val_pred, average='micro'))

print('í›ˆë ¨ f1 score (macro): ', f1_score(y_train, y_train_pred, average='macro'))
print('ê²€ì¦ f1 score (macro): ', f1_score(y_val, y_val_pred, average='macro'))


# In[ ]:


pd.DataFrame(clf.cv_results_).sort_values(by='rank_test_score').T


# In[ ]:


# Modeling

dists = { # Randomized Classifier Parameter
  'randomforestclassifier__n_estimators' : randint(100, 300),
  'randomforestclassifier__max_depth' : randint(10,21),
  'randomforestclassifier__min_samples_leaf' : [5,6],
  'randomforestclassifier__min_samples_split' : randint(5, 10),
  'randomforestclassifier__max_features' : randint(3,5)
}


pipe = make_pipeline( # Modeling Pipeline
    OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
    SimpleImputer(missing_values=np.nan, strategy='mean'),
    RandomForestClassifier(random_state=2)
)

clf = RandomizedSearchCV( # RamdommizedSearchCV
    pipe, 
    param_distributions=dists, 
    n_iter=10, 
    cv=3, 
    scoring='accuracy',  
    verbose=1,
    n_jobs=-1
)


clf.fit(X_train, y_train)

y_pred = clf.predict(X_val)

print(classification_report(y_val, y_pred))

# ì‹œê°í™”
label = ['Attack_C', 'Defence_C', 'Midfielder_C', 'Def_Midfielder', 'GK',
         'Attack_L', 'Defence_L', 'Midfielder_L', 'Attack_R', 'Defence_R', 'Midfielder_R']
fig, ax = plt.subplots(figsize=(15, 15))
plot_confusion_matrix(clf, 
                              X_val, y_val,
                              display_labels = label,
                              cmap = 'Blues',
                              normalize='true',
                              ax = ax)
plt.title('Random Forest Classifier (Position)', fontsize=15);


# In[ ]:


model = clf.best_estimator_

y_val_pred = model.predict(X_val)
y_train_pred = model.predict(X_train)

print('í›ˆë ¨ f1 score (micro): ', f1_score(y_train, y_train_pred, average='micro'))
print('ê²€ì¦ f1 score (micro): ', f1_score(y_val, y_val_pred, average='micro'))

print('í›ˆë ¨ f1 score (macro): ', f1_score(y_train, y_train_pred, average='macro'))
print('ê²€ì¦ f1 score (macro): ', f1_score(y_val, y_val_pred, average='macro'))


# In[ ]:


pd.DataFrame(clf.cv_results_).sort_values(by='rank_test_score').T


# In[ ]:


# Modeling

dists = { # Randomized Classifier Parameter
  'randomforestclassifier__n_estimators' : randint(80, 200),
  'randomforestclassifier__max_depth' : randint(5,15),
  'randomforestclassifier__min_samples_leaf' : [5,6],
  'randomforestclassifier__min_samples_split' : randint(5, 10),
  'randomforestclassifier__max_features' : randint(3,6)
}


pipe = make_pipeline( # Modeling Pipeline
    OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
    SimpleImputer(missing_values=np.nan, strategy='mean'),
    RandomForestClassifier(random_state=2)
)

clf = RandomizedSearchCV( # RamdommizedSearchCV
    pipe, 
    param_distributions=dists, 
    n_iter=10, 
    cv=3, 
    scoring='accuracy',  
    verbose=1,
    n_jobs=-1
)


clf.fit(X_train, y_train)

y_pred = clf.predict(X_val)

print(classification_report(y_val, y_pred))

# ì‹œê°í™”
label = ['Attack_C', 'Defence_C', 'Midfielder_C', 'Def_Midfielder', 'GK',
         'Attack_L', 'Defence_L', 'Midfielder_L', 'Attack_R', 'Defence_R', 'Midfielder_R']
fig, ax = plt.subplots(figsize=(15, 15))
plot_confusion_matrix(clf, 
                              X_val, y_val,
                              display_labels = label,
                              cmap = 'Blues',
                              normalize='true',
                              ax = ax)
plt.title('Random Forest Classifier (Position)', fontsize=15);


# In[ ]:


model = clf.best_estimator_

y_val_pred = model.predict(X_val)
y_train_pred = model.predict(X_train)

print('í›ˆë ¨ f1 score (micro): ', f1_score(y_train, y_train_pred, average='micro'))
print('ê²€ì¦ f1 score (micro): ', f1_score(y_val, y_val_pred, average='micro'))

print('í›ˆë ¨ f1 score (macro): ', f1_score(y_train, y_train_pred, average='macro'))
print('ê²€ì¦ f1 score (macro): ', f1_score(y_val, y_val_pred, average='macro'))


# In[ ]:


pd.DataFrame(clf.cv_results_).sort_values(by='rank_test_score').T


# In[ ]:


# Modeling

dists = { # Randomized Classifier Parameter
  'randomforestclassifier__n_estimators' : randint(100, 200),
  'randomforestclassifier__max_depth' : randint(10,14),
  'randomforestclassifier__min_samples_leaf' : randint(5,10),
  'randomforestclassifier__min_samples_split' : randint(7, 10),
  'randomforestclassifier__max_features' : randint(3,8)
}


pipe = make_pipeline( # Modeling Pipeline
    OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
    SimpleImputer(missing_values=np.nan, strategy='mean'),
    RandomForestClassifier(random_state=2)
)

clf = RandomizedSearchCV( # RamdommizedSearchCV
    pipe, 
    param_distributions=dists, 
    n_iter=12, 
    cv=3, 
    scoring='accuracy',  
    verbose=1,
    n_jobs=-1
)


clf.fit(X_train, y_train)

y_pred = clf.predict(X_val)

print(classification_report(y_val, y_pred))

# ì‹œê°í™”
label = ['Attack_C', 'Defence_C', 'Midfielder_C', 'Def_Midfielder', 'GK',
         'Attack_L', 'Defence_L', 'Midfielder_L', 'Attack_R', 'Defence_R', 'Midfielder_R']
fig, ax = plt.subplots(figsize=(15, 15))
plot_confusion_matrix(clf, 
                              X_val, y_val,
                              display_labels = label,
                              cmap = 'Blues',
                              normalize='true',
                              ax = ax)
plt.title('Random Forest Classifier (Position)', fontsize=15);


# In[ ]:


model = clf.best_estimator_

y_val_pred = model.predict(X_val)
y_train_pred = model.predict(X_train)

print('í›ˆë ¨ f1 score (micro): ', f1_score(y_train, y_train_pred, average='micro'))
print('ê²€ì¦ f1 score (micro): ', f1_score(y_val, y_val_pred, average='micro'))

print('í›ˆë ¨ f1 score (macro): ', f1_score(y_train, y_train_pred, average='macro'))
print('ê²€ì¦ f1 score (macro): ', f1_score(y_val, y_val_pred, average='macro'))

pd.DataFrame(clf.cv_results_).sort_values(by='rank_test_score').T


# In[ ]:


# Modeling

dists = { # Randomized Classifier Parameter
  'xgbclassifier__n_estimators' : randint(130,180),
}


pipe = make_pipeline( # Modeling Pipeline
    OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
    SimpleImputer(missing_values=np.nan, strategy='mean'),
    XGBClassifier(random_state=2)
)

clf = RandomizedSearchCV( # RamdommizedSearchCV
    pipe, 
    param_distributions=dists, 
    n_iter=3, 
    cv=3, 
    scoring='accuracy',  
    verbose=1,
    n_jobs=-1
)


clf.fit(X_train, y_train)

y_pred = clf.predict(X_val)

print(classification_report(y_val, y_pred))

# ì‹œê°í™”
label = ['Attack_C', 'Defence_C', 'Midfielder_C', 'Def_Midfielder', 'GK',
         'Attack_L', 'Defence_L', 'Midfielder_L', 'Attack_R', 'Defence_R', 'Midfielder_R']
fig, ax = plt.subplots(figsize=(15, 15))
plot_confusion_matrix(clf, 
                              X_val, y_val,
                              display_labels = label,
                              cmap = 'Blues',
                              normalize='true',
                              ax = ax)
plt.title('Random Forest Classifier (Position)', fontsize=15);


# In[ ]:


model = clf.best_estimator_

y_val_pred = model.predict(X_val)
y_train_pred = model.predict(X_train)

print('í›ˆë ¨ f1 score (micro): ', f1_score(y_train, y_train_pred, average='micro'))
print('ê²€ì¦ f1 score (micro): ', f1_score(y_val, y_val_pred, average='micro'))

print('í›ˆë ¨ f1 score (macro): ', f1_score(y_train, y_train_pred, average='macro'))
print('ê²€ì¦ f1 score (macro): ', f1_score(y_val, y_val_pred, average='macro'))

pd.DataFrame(clf.cv_results_).sort_values(by='rank_test_score').T


# In[ ]:


dists = { # Randomized Classifier Parameter
  'xgbclassifier__n_estimators' : randint(150,180),
}


pipe = make_pipeline( # Modeling Pipeline
    OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
    SimpleImputer(missing_values=np.nan, strategy='mean'),
    XGBClassifier(random_state=2)
)

clf = RandomizedSearchCV( # RamdommizedSearchCV
    pipe, 
    param_distributions=dists, 
    n_iter=3, 
    cv=3, 
    scoring='accuracy',  
    verbose=1,
    n_jobs=-1
)


clf.fit(X_train, y_train)

y_pred = clf.predict(X_val)

print(classification_report(y_val, y_pred))

# ì‹œê°í™”
label = ['Attack_C', 'Defence_C', 'Midfielder_C', 'Def_Midfielder', 'GK',
         'Attack_L', 'Defence_L', 'Midfielder_L', 'Attack_R', 'Defence_R', 'Midfielder_R']
fig, ax = plt.subplots(figsize=(15, 15))
plot_confusion_matrix(clf, 
                              X_val, y_val,
                              display_labels = label,
                              cmap = 'Blues',
                              normalize='true',
                              ax = ax)
plt.title('Random Forest Classifier (Position)', fontsize=15);


# In[ ]:


model = clf.best_estimator_

y_val_pred = model.predict(X_val)
y_train_pred = model.predict(X_train)

print('í›ˆë ¨ f1 score (micro): ', f1_score(y_train, y_train_pred, average='micro'))
print('ê²€ì¦ f1 score (micro): ', f1_score(y_val, y_val_pred, average='micro'))

print('í›ˆë ¨ f1 score (macro): ', f1_score(y_train, y_train_pred, average='macro'))
print('ê²€ì¦ f1 score (macro): ', f1_score(y_val, y_val_pred, average='macro'))

pd.DataFrame(clf.cv_results_).sort_values(by='rank_test_score').T


# In[ ]:


dists = { # Randomized Classifier Parameter
  'xgbclassifier__n_estimators' : randint(160,178),
}


pipe = make_pipeline( # Modeling Pipeline
    OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
    SimpleImputer(missing_values=np.nan, strategy='mean'),
    XGBClassifier(random_state=2)
)

clf = RandomizedSearchCV( # RamdommizedSearchCV
    pipe, 
    param_distributions=dists, 
    n_iter=3, 
    cv=3, 
    scoring='accuracy',  
    verbose=1,
    n_jobs=-1
)


clf.fit(X_train, y_train)

y_pred = clf.predict(X_val)

print(classification_report(y_val, y_pred))

# ì‹œê°í™”
label = ['Attack_C', 'Defence_C', 'Midfielder_C', 'Def_Midfielder', 'GK',
         'Attack_L', 'Defence_L', 'Midfielder_L', 'Attack_R', 'Defence_R', 'Midfielder_R']
fig, ax = plt.subplots(figsize=(15, 15))
plot_confusion_matrix(clf, 
                              X_val, y_val,
                              display_labels = label,
                              cmap = 'Blues',
                              normalize='true',
                              ax = ax)
plt.title('XGBoost Classifier (Position)', fontsize=15);


# In[ ]:


model = clf.best_estimator_

y_val_pred = model.predict(X_val)
y_train_pred = model.predict(X_train)

print('í›ˆë ¨ f1 score (micro): ', f1_score(y_train, y_train_pred, average='micro'))
print('ê²€ì¦ f1 score (micro): ', f1_score(y_val, y_val_pred, average='micro'))

print('í›ˆë ¨ f1 score (macro): ', f1_score(y_train, y_train_pred, average='macro'))
print('ê²€ì¦ f1 score (macro): ', f1_score(y_val, y_val_pred, average='macro'))

pd.DataFrame(clf.cv_results_).sort_values(by='rank_test_score').T


# In[ ]:


param = { # Randomized Classifier Parameter
  'xgbclassifier__n_estimators' : (166,169,171,174,175,178),
}


pipe = make_pipeline( # Modeling Pipeline
    OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
    SimpleImputer(missing_values=np.nan, strategy='mean'),
    XGBClassifier(random_state=2)
)

clf = GridSearchCV( # RamdommizedSearchCV
    pipe, 
    param_grid=param, 
    cv=3, 
    scoring='accuracy',  
    verbose=1,
    n_jobs=-1
)


clf.fit(X_train, y_train)

y_pred = clf.predict(X_val)

print(classification_report(y_val, y_pred))

# ì‹œê°í™”
label = ['Attack_C', 'Defence_C', 'Midfielder_C', 'Def_Midfielder', 'GK',
         'Attack_L', 'Defence_L', 'Midfielder_L', 'Attack_R', 'Defence_R', 'Midfielder_R']
fig, ax = plt.subplots(figsize=(15, 15))
plot_confusion_matrix(clf, 
                              X_val, y_val,
                              display_labels = label,
                              cmap = 'Blues',
                              normalize='true',
                              ax = ax)
plt.title('XGBoost Classifier (Position)', fontsize=15);


# In[ ]:


model = clf.best_estimator_

y_val_pred = model.predict(X_val)
y_train_pred = model.predict(X_train)

print('í›ˆë ¨ f1 score (micro): ', f1_score(y_train, y_train_pred, average='micro'))
print('ê²€ì¦ f1 score (micro): ', f1_score(y_val, y_val_pred, average='micro'))

print('í›ˆë ¨ f1 score (macro): ', f1_score(y_train, y_train_pred, average='macro'))
print('ê²€ì¦ f1 score (macro): ', f1_score(y_val, y_val_pred, average='macro'))

pd.DataFrame(clf.cv_results_).sort_values(by='rank_test_score').T


# In[ ]:


pipe = make_pipeline( # Modeling Pipeline
    OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
    SimpleImputer(missing_values=np.nan, strategy='mean'),
    XGBClassifier(random_state=2, n_estmators=166)
)

pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)
print(classification_report(y_test, y_pred))


# # Permutation importances
# <aside>
# 
# ###ğŸ’¡ **5) ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•´ì„**
# 
# í”„ë¡œì íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆëŠ” ë¶€ë¶„ ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” SHAP, PDP ë“±ì„ í†µí•´ì„œ ëª¨ë¸ì´ ê´€ì¸¡ì¹˜ë¥¼ ì–´ë–¤ íŠ¹ì„±ì„ í™œìš©í–ˆê±°ë‚˜, ì–´ë–¤ íŠ¹ì„±ì´ íƒ€ê²Ÿì— ì˜í–¥ì„ ë¼ì³¤ëŠ”ì§€ ë“±ì„ í•´ì„í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ë°°ì› ìŠµë‹ˆë‹¤.
# ì—¬ëŸ¬ë¶„ì˜ í”„ë¡œì íŠ¸ì—ë„ ì´ëŸ¬í•œ í•´ì„ ë°©ë²•ì„ í™œìš©í•´ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ë¹„ì „ë¬¸ê°€ë¼ë„ ì¡°ê¸ˆ ë” ì‰½ê²Œ ì´í•´í•˜ê³  ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ì…”ì•¼ í•©ë‹ˆë‹¤.
# 
# - PDP, SHAPì„ í™œìš©í•˜ì—¬ ìµœì¢… ëª¨ë¸ì„ ì„¤ëª…í•©ë‹ˆë‹¤
# - ì‹œê°í™”ëŠ” "ì„¤ëª…"ì´ ì œì¼ ì¤‘ìš”í•©ë‹ˆë‹¤.
# 
# ### **íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•œ í›„, ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€ë‹µí•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.**
# 
# 1. ëª¨ë¸ì´ ê´€ì¸¡ì¹˜ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œ ì–´ë–¤ íŠ¹ì„±ì„ í™œìš©í–ˆë‚˜ìš”?
# 2. ì–´ë–¤ íŠ¹ì„±ì´ ìˆë‹¤ë©´ ëª¨ë¸ì˜ ì˜ˆì¸¡ì— ë„ì›€ì´ ë ê¹Œìš”? í•´ë‹¹ íŠ¹ì„±ì€ ì–´ë–»ê²Œ êµ¬í•  ìˆ˜ ìˆì„ê¹Œìš”?
# 
# train_features_transform
# </aside>

# In[12]:


get_ipython().system('pip install eli5')


# In[13]:


pipe = Pipeline([
                 ('preprocessing', make_pipeline(
                     OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
                     SimpleImputer(missing_values=np.nan, strategy='mean')
                 )),
                 ('clf',  XGBClassifier(random_state=2, n_estmators=166))
],verbose=1)

pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)


# In[14]:


import eli5
from eli5.sklearn import PermutationImportance

permuter = PermutationImportance(
    pipe.named_steps['clf'],
    scoring='accuracy',
    n_iter=5,
    random_state=2
)

X_test_transformed = pipe.named_steps['preprocessing'].transform(X_test)
permuter.fit(X_test_transformed, y_test);

feature_names = train_features_transform.tolist() #Xvalì˜ ì»¬ëŸ¼ *ë¦¬ìŠ¤íŠ¸í™”*
pd.Series(permuter.feature_importances_, feature_names).sort_values()


# In[15]:


# íŠ¹ì„±ë³„ score í™•ì¸
eli5.show_weights(
    permuter, 
    top=None, # top n ì§€ì • ê°€ëŠ¥, None ì¼ ê²½ìš° ëª¨ë“  íŠ¹ì„± 
    feature_names=feature_names # list í˜•ì‹ìœ¼ë¡œ ë„£ì–´ì•¼ í•©ë‹ˆë‹¤
)


# In[17]:


print('íŠ¹ì„± ì‚­ì œ ì „:', X_train.shape, X_val.shape)

minimum_importance = 0.000 # ìµœì†Œ ì¤‘ìš”ë„ (ìµœì†Œ ì´ì •ë„ ì´ìƒì€ ë„˜ì–´ì•¼ í•œë‹¤.)
mask = permuter.feature_importances_ > minimum_importance
features = train_features_transform[mask]

X_train_selected = enc_train[features]
X_val_selected = enc_val[features]
X_test_selected = enc_test[features]

print('íŠ¹ì„± ì‚­ì œ í›„:', X_train_selected.shape, X_val_selected.shape)


# In[20]:


pipe = Pipeline([
                 ('preprocessing', make_pipeline(
#                     OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
                     SimpleImputer(missing_values=np.nan, strategy='mean')
                 )),
                 ('clf',  XGBClassifier(random_state=2, n_estmators=166))
],verbose=1)

pipe.fit(X_train_selected, y_train)
y_pred = pipe.predict(X_val_selected)

y_train_pred = pipe.predict(X_train_selected)

print(classification_report(y_val, y_pred))

print('í›ˆë ¨ f1 score (micro): ', f1_score(y_train, y_train_pred, average='micro'))
print('ê²€ì¦ f1 score (micro): ', f1_score(y_val, y_pred, average='micro'))

print('í›ˆë ¨ f1 score (macro): ', f1_score(y_train, y_train_pred, average='macro'))
print('ê²€ì¦ f1 score (macro): ', f1_score(y_val, y_pred, average='macro'))


# In[22]:


y_pred = pipe.predict(X_test_selected)

y_train_pred = pipe.predict(X_train_selected)

print(classification_report(y_test, y_pred))

print('í›ˆë ¨ f1 score (micro): ', f1_score(y_train, y_train_pred, average='micro'))
print('í…ŒìŠ¤íŠ¸ f1 score (micro): ', f1_score(y_test, y_pred, average='micro'))

print('í›ˆë ¨ f1 score (macro): ', f1_score(y_train, y_train_pred, average='macro'))
print('í…ŒìŠ¤íŠ¸ f1 score (macro): ', f1_score(y_test, y_pred, average='macro'))
print('---------------')
print('í›ˆë ¨ accuracy (micro): ', accuracy_score(y_train, y_train_pred))
print('í…ŒìŠ¤íŠ¸ accuracy (micro): ', accuracy_score(y_test, y_pred))

print('í›ˆë ¨ accuracy (macro): ', accuracy_score(y_train, y_train_pred))
print('í…ŒìŠ¤íŠ¸ accuracy (macro): ', accuracy_score(y_test, y_pred))

# ì‹œê°í™”
label = ['Attack_C', 'Defence_C', 'Midfielder_C', 'Def_Midfielder', 'GK',
         'Attack_L', 'Defence_L', 'Midfielder_L', 'Attack_R', 'Defence_R', 'Midfielder_R']
fig, ax = plt.subplots(figsize=(15, 15))
plot_confusion_matrix(pipe, 
                              X_test_selected, y_test,
                              display_labels = label,
                              cmap = 'Blues',
                              normalize='true',
                              ax = ax)

plt.title('XGBoost Classifier (Position)', fontsize=15);


# In[23]:


get_ipython().system('pip install shap')


# In[47]:


enc = OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True)
imp = SimpleImputer(missing_values=np.nan, strategy='mean')

clf = XGBClassifier(random_state=2, n_estmators=166)

X_train_transformed = enc.fit_transform(X_train)
X_test_transformed = enc.transform(X_test)
X_train_imputed = imp.fit_transform(X_train_transformed)
X_test_imputed = imp.transform(X_test_transformed)

clf.fit(X_train_imputed, y_train)
y_pred = clf.predict(X_test_imputed)


# In[43]:


dataset = pd.DataFrame(X_test_transformed)


# In[44]:


dataset.columns = enc_test.columns


# In[53]:


import shap

explainer = shap.TreeExplainer(clf) 
shap.initjs() #ì¸ë¼ì¸ ìë°”ìŠ¤í¬ë¦½íŠ¸

shap_values = explainer.shap_values(dataset.iloc[:200])
shap.summary_plot(shap_values, dataset.iloc[:200], plot_type='bar', max_display=6)


# In[37]:


X_train_selected


# # 4 Positions

# In[69]:


df = pd.read_csv('/content/cls_df.csv')
df = df.iloc[:,1:]


# In[70]:


df


# In[71]:


# ê³µê²©ìˆ˜
df.loc[df['position']=='ST', ['position']] = 'Attack'
df.loc[df['position']=='LS', ['position']] = 'Attack'
df.loc[df['position']=='RS', ['position']] = 'Attack'
df.loc[df['position']=='RF', ['position']] = 'Attack'
df.loc[df['position']=='LF', ['position']] = 'Attack'
df.loc[df['position']=='CF', ['position']] = 'Attack'
df.loc[df['position']=='LW', ['position']] = 'Attack'
df.loc[df['position']=='RW', ['position']] = 'Attack'

# ê³¨ê¸°í¼
df.loc[df['position']=='GK', ['position']] = 'Goalkeeper'

# ë¯¸ë“œí•„ë”
df.loc[df['position']=='CAM', ['position']] = 'Middle'
df.loc[df['position']=='LAM', ['position']] = 'Middle'
df.loc[df['position']=='RAM', ['position']] = 'Middle'
df.loc[df['position']=='AM', ['position']] = 'Middle'
df.loc[df['position']=='LM', ['position']] = 'Middle'
df.loc[df['position']=='RM', ['position']] = 'Middle'
df.loc[df['position']=='CM', ['position']] = 'Middle'
df.loc[df['position']=='LCM', ['position']] = 'Middle'
df.loc[df['position']=='RCM', ['position']] = 'Middle'
df.loc[df['position']=='CDM', ['position']] = 'Middle'
df.loc[df['position']=='LDM', ['position']] = 'Middle'
df.loc[df['position']=='RDM', ['position']] = 'Middle'

# ìˆ˜ë¹„ìˆ˜
df.loc[df['position']=='LWB', ['position']] = 'Defence'
df.loc[df['position']=='RWB', ['position']] = 'Defence'
df.loc[df['position']=='CB', ['position']] = 'Defence'
df.loc[df['position']=='LCB', ['position']] = 'Defence'
df.loc[df['position']=='RCB', ['position']] = 'Defence'
df.loc[df['position']=='LB', ['position']] = 'Defence'
df.loc[df['position']=='RB', ['position']] = 'Defence'


# In[72]:


df.to_csv('/content/cls4_df.csv', index=True)


# In[75]:


from sklearn.model_selection import train_test_split 

target = 'position'
train, test = train_test_split(df, random_state=2, train_size=.75, stratify=df[target])
train, val = train_test_split(train, random_state=2, train_size=.75, stratify=train[target])
train.shape, val.shape, test.shape


# In[76]:


features = train.columns.drop(target)
X_train = train[features] 
X_val = val[features]
X_test = test[features]
y_train = train[target]
y_val = val[target] 
y_test = test[target]


# In[77]:


enc = OneHotEncoder(cols = [
                            'preferred_foot',
                            'body_type'
], use_cat_names=True)

enc_train = enc.fit_transform(X_train)
train_features_transform = enc_train.columns # ì¸ì½”ë”©ì´ ì™„ë£Œëœ íŠ¹ì„±ì„ ë³€ìˆ˜ì— ë‹´ê¸°

enc_val = enc.transform(X_val)
enc_test = enc.transform(X_test)


# In[84]:


param = { # Randomized Classifier Parameter
  'xgbclassifier__n_estimators' : (166,169,171,174,175,178),
}


pipe = make_pipeline( # Modeling Pipeline
    OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
    SimpleImputer(missing_values=np.nan, strategy='mean'),
    XGBClassifier(random_state=2)
)

clf = GridSearchCV( # RamdommizedSearchCV
    pipe, 
    param_grid=param, 
    cv=3, 
    scoring='accuracy',  
    verbose=1,
    n_jobs=-1
)


clf.fit(X_train, y_train)

y_pred = clf.predict(X_val)

print(classification_report(y_val, y_pred))


# In[85]:


# ì‹œê°í™”
label = ['Attack', 'Defence', 'Goalkeeper','Middle']
fig, ax = plt.subplots(figsize=(15, 15))
plot_confusion_matrix(clf, 
                              X_val, y_val,
                              display_labels = label,
                              cmap = 'Blues',
                              normalize='true',
                              ax = ax)
plt.title('XGBoost Classifier (Position)', fontsize=15);


# In[86]:


model = clf.best_estimator_

y_val_pred = model.predict(X_val)
y_train_pred = model.predict(X_train)

print('í›ˆë ¨ f1 score (micro): ', f1_score(y_train, y_train_pred, average='micro'))
print('ê²€ì¦ f1 score (micro): ', f1_score(y_val, y_val_pred, average='micro'))

print('í›ˆë ¨ f1 score (macro): ', f1_score(y_train, y_train_pred, average='macro'))
print('ê²€ì¦ f1 score (macro): ', f1_score(y_val, y_val_pred, average='macro'))

pd.DataFrame(clf.cv_results_).sort_values(by='rank_test_score').T


# In[88]:


pipe = make_pipeline( # Modeling Pipeline
    OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
    SimpleImputer(missing_values=np.nan, strategy='mean'),
    XGBClassifier(random_state=2, n_estmators=178)
)

pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)
print(classification_report(y_test, y_pred))


# In[91]:


y_pred = pipe.predict(X_test)

y_train_pred = pipe.predict(X_train)

print(classification_report(y_test, y_pred))

print('í›ˆë ¨ f1 score (micro): ', f1_score(y_train, y_train_pred, average='micro'))
print('í…ŒìŠ¤íŠ¸ f1 score (micro): ', f1_score(y_test, y_pred, average='micro'))

print('í›ˆë ¨ f1 score (macro): ', f1_score(y_train, y_train_pred, average='macro'))
print('í…ŒìŠ¤íŠ¸ f1 score (macro): ', f1_score(y_test, y_pred, average='macro'))
print('---------------')
print('í›ˆë ¨ accuracy (micro): ', accuracy_score(y_train, y_train_pred))
print('í…ŒìŠ¤íŠ¸ accuracy (micro): ', accuracy_score(y_test, y_pred))

print('í›ˆë ¨ accuracy (macro): ', accuracy_score(y_train, y_train_pred))
print('í…ŒìŠ¤íŠ¸ accuracy (macro): ', accuracy_score(y_test, y_pred))


# ì‹œê°í™”
label = ['Attack', 'Defence', 'Goalkeeper','Middle']
fig, ax = plt.subplots(figsize=(15, 15))
plot_confusion_matrix(clf, 
                              X_val, y_val,
                              display_labels = label,
                              cmap = 'Blues',
                              normalize='true',
                              ax = ax)
plt.title('XGBoost Classifier (Position)', fontsize=15);


# In[93]:


pipe = Pipeline([
                 ('preprocessing', make_pipeline(
                     OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
                     SimpleImputer(missing_values=np.nan, strategy='mean')
                 )),
                 ('clf',  XGBClassifier(random_state=2, n_estmators=178))
],verbose=1)

pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)


# In[94]:


import eli5
from eli5.sklearn import PermutationImportance

permuter = PermutationImportance(
    pipe.named_steps['clf'],
    scoring='accuracy',
    n_iter=5,
    random_state=2
)

X_test_transformed = pipe.named_steps['preprocessing'].transform(X_test)
permuter.fit(X_test_transformed, y_test);

feature_names = train_features_transform.tolist() #Xvalì˜ ì»¬ëŸ¼ *ë¦¬ìŠ¤íŠ¸í™”*
pd.Series(permuter.feature_importances_, feature_names).sort_values()


# In[95]:


# íŠ¹ì„±ë³„ score í™•ì¸
eli5.show_weights(
    permuter, 
    top=None, # top n ì§€ì • ê°€ëŠ¥, None ì¼ ê²½ìš° ëª¨ë“  íŠ¹ì„± 
    feature_names=feature_names # list í˜•ì‹ìœ¼ë¡œ ë„£ì–´ì•¼ í•©ë‹ˆë‹¤
)


# In[96]:


print('íŠ¹ì„± ì‚­ì œ ì „:', X_train.shape, X_val.shape)

minimum_importance = 0.000 # ìµœì†Œ ì¤‘ìš”ë„ (ìµœì†Œ ì´ì •ë„ ì´ìƒì€ ë„˜ì–´ì•¼ í•œë‹¤.)
mask = permuter.feature_importances_ > minimum_importance
features = train_features_transform[mask]

X_train_selected = enc_train[features]
X_val_selected = enc_val[features]
X_test_selected = enc_test[features]

print('íŠ¹ì„± ì‚­ì œ í›„:', X_train_selected.shape, X_val_selected.shape)


# In[97]:


pipe = Pipeline([
                 ('preprocessing', make_pipeline(
#                     OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True),
                     SimpleImputer(missing_values=np.nan, strategy='mean')
                 )),
                 ('clf',  XGBClassifier(random_state=2, n_estmators=166))
],verbose=1)

pipe.fit(X_train_selected, y_train)
y_pred = pipe.predict(X_val_selected)

y_train_pred = pipe.predict(X_train_selected)

print(classification_report(y_val, y_pred))

print('í›ˆë ¨ f1 score (micro): ', f1_score(y_train, y_train_pred, average='micro'))
print('ê²€ì¦ f1 score (micro): ', f1_score(y_val, y_pred, average='micro'))

print('í›ˆë ¨ f1 score (macro): ', f1_score(y_train, y_train_pred, average='macro'))
print('ê²€ì¦ f1 score (macro): ', f1_score(y_val, y_pred, average='macro'))


# In[99]:


y_pred = pipe.predict(X_test_selected)

y_train_pred = pipe.predict(X_train_selected)

print(classification_report(y_test, y_pred))

print('í›ˆë ¨ f1 score (micro): ', f1_score(y_train, y_train_pred, average='micro'))
print('í…ŒìŠ¤íŠ¸ f1 score (micro): ', f1_score(y_test, y_pred, average='micro'))

print('í›ˆë ¨ f1 score (macro): ', f1_score(y_train, y_train_pred, average='macro'))
print('í…ŒìŠ¤íŠ¸ f1 score (macro): ', f1_score(y_test, y_pred, average='macro'))
print('---------------')
print('í›ˆë ¨ accuracy (micro): ', accuracy_score(y_train, y_train_pred))
print('í…ŒìŠ¤íŠ¸ accuracy (micro): ', accuracy_score(y_test, y_pred))

print('í›ˆë ¨ accuracy (macro): ', accuracy_score(y_train, y_train_pred))
print('í…ŒìŠ¤íŠ¸ accuracy (macro): ', accuracy_score(y_test, y_pred))

# ì‹œê°í™”
label = ['Attack', 'Defence', 'Goalkeeper','Middle']
fig, ax = plt.subplots(figsize=(10, 10))
plot_confusion_matrix(pipe, 
                              X_test_selected, y_test,
                              display_labels = label,
                              cmap = 'Blues',
                              normalize='true',
                              ax = ax)

plt.title('XGBoost Classifier (Position)', fontsize=15);


# In[100]:


enc = OneHotEncoder(cols=['preferred_foot','body_type'], use_cat_names=True)
imp = SimpleImputer(missing_values=np.nan, strategy='mean')

clf = XGBClassifier(random_state=2, n_estmators=166)

X_train_transformed = enc.fit_transform(X_train)
X_test_transformed = enc.transform(X_test)
X_train_imputed = imp.fit_transform(X_train_transformed)
X_test_imputed = imp.transform(X_test_transformed)

clf.fit(X_train_imputed, y_train)
y_pred = clf.predict(X_test_imputed)


# In[101]:


dataset = pd.DataFrame(X_test_transformed)


# In[102]:


dataset.columns = enc_test.columns


# In[104]:


import shap

explainer = shap.TreeExplainer(clf) 
shap.initjs() #ì¸ë¼ì¸ ìë°”ìŠ¤í¬ë¦½íŠ¸

shap_values = explainer.shap_values(dataset.iloc[:200])
shap.summary_plot(shap_values, dataset.iloc[:200], plot_type='bar', max_display=6)


# In[ ]:




