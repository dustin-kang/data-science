{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 얼굴 인식하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9a4bc8903fc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_detection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscaleFactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mminNeighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mminSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCASCADE_SCALE_IMAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mcanvas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"uint8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#모듈 임포트\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import imutils # OpenCV가 제공하는 기능 중에 좀 복잡하고 사용성이 떨어지는 부분을 잘 보완해 주는 패키지\n",
    "import cv2 \n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# 모델 가져오기\n",
    "detection_model_path = 'haarcascade_files/haarcascade_frontalface_default.xml'\n",
    "#얼굴 감지 모델(haarcascade 트레이닝 데이터를 가져온다.)\n",
    "emotion_model_path = 'models/_mini_XCEPTION.102-0.66.hdf5'\n",
    "#표정 감지 모델\n",
    "\n",
    "# 바운딩 박스 프레임의 하이퍼 파라미터 설정\n",
    "# loading models\n",
    "face_detection = cv2.CascadeClassifier(detection_model_path)\n",
    "#얼굴 감지 모델을 가져와서 CascadeClassifier 객체를 생성한다.\n",
    "emotion_classifier = load_model(emotion_model_path, compile=False)\n",
    "#감정 감지 모델을 가져와서 분류 모델 객체 생성\n",
    "EMOTIONS = [\"angry\" ,\"disgust\",\"scared\", \"happy\", \"sad\", \"surprised\",\n",
    " \"neutral\"]\n",
    "#데이터셋의 특징\n",
    "\n",
    "\n",
    "#feelings_faces = []\n",
    "#for index, emotion in enumerate(EMOTIONS):\n",
    "   # feelings_faces.append(cv2.imread('emojis/' + emotion + '.png', -1))\n",
    "\n",
    "# starting video streaming\n",
    "cv2.namedWindow('your_face') #창 이름\n",
    "camera = cv2.VideoCapture(0) #비디오 ON! 0 = 첫번째 인덱스 카메라\n",
    "while True: #무한 반복\n",
    "    frame = camera.read()[1] #카메라를 읽는다.\n",
    "    #카메라 창 읽기\n",
    "    frame = imutils.resize(frame,width=300) #카메라(frame)사이즈 조정\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #cvtColor 함수를 이용하여 기본 BGR컬러를 Gray로 바꾼다.\n",
    "    faces = face_detection.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    #detectMultiScale에 grayscale 이미지를 입력하여 얼굴을 검출한다. 검출되면 위치로 리스트한다. 링크참조.\n",
    "    canvas = np.zeros((250, 300, 3), dtype=\"uint8\") #canvas 사이즈\n",
    "    frameClone = frame.copy() #FrameClone <- Frame (배열복제)\n",
    "    if len(faces) > 0: #얼굴 검출 설정 (얼굴 영역)\n",
    "        faces = sorted(faces, reverse=True,\n",
    "        key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))[0]\n",
    "        (fX, fY, fW, fH) = faces\n",
    "                    # 그레이스케일 영상에서 얼굴의 roi를 추출하고 고정된 28x28 픽셀로 크기를 조정한 다음 준비한다.\n",
    "            # CNN을 통한 roi 분류\n",
    "        roi = gray[fY:fY + fH, fX:fX + fW] #roi(관심영역 지정)\n",
    "        roi = cv2.resize(roi, (64, 64)) # 관심영역 사이즈\n",
    "        roi = roi.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0) \n",
    "        \n",
    "        \n",
    "        preds = emotion_classifier.predict(roi)[0]\n",
    "        emotion_probability = np.max(preds)\n",
    "        label = EMOTIONS[preds.argmax()]\n",
    "    else: continue\n",
    "\n",
    " \n",
    "    for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds)):\n",
    "                # 라벨의 문자 설정\n",
    "                text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
    "\n",
    "                # cavas 창에 라벨과 확률치 바를 만든다.\n",
    "               # emoji_face = feelings_faces[np.argmax(preds)]\n",
    "\n",
    "                \n",
    "                w = int(prob * 300)\n",
    "                cv2.rectangle(canvas, (7, (i * 35) + 5), #확률치 바\n",
    "                (w, (i * 35) + 35), (0, 0, 255), -1)\n",
    "                cv2.putText(canvas, text, (10, (i * 35) + 23), #텍스트\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.45, #폰트 설정\n",
    "                (255, 255, 255), 2)\n",
    "                cv2.putText(frameClone, label, (fX, fY - 10), #카메라에 텍스트 관심영역 -10 (위로 10)\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2) #폰트 설정\n",
    "                cv2.rectangle(frameClone, (fX, fY), (fX + fW, fY + fH),\n",
    "                              (0, 0, 255), 2) #카메라의 프레임(관심영역)\n",
    "#    for c in range(0, 3):\n",
    "#        frame[200:320, 10:130, c] = emoji_face[:, :, c] * \\\n",
    "#        (emoji_face[:, :, 3] / 255.0) + frame[200:320,\n",
    "#        10:130, c] * (1.0 - emoji_face[:, :, 3] / 255.0)\n",
    "\n",
    "\n",
    "    cv2.imshow('your_face', frameClone) #카메라창(frameClone)을 출력한다.\n",
    "    cv2.imshow(\"Probabilities\", canvas) #특성창(canvas)를 출력한다..\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): #Q를 누르면 나간다.\n",
    "        break\n",
    "\n",
    "camera.release() #카메라를 닫는다.\n",
    "cv2.destroyAllWindows() #윈도우 창을 끈다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
